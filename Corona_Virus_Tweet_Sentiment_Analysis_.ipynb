{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "H0kj-8xxnORC",
        "MSa1f5Uengrz",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhrisandamedhi/Corona-Virus-Tweet-Sentiment-/blob/main/Corona_Virus_Tweet_Sentiment_Analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Corona Virus Tweet Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Classification\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### **Contribution**    - Individual\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### **Team Member 1 -** Dhrisanda Medhi\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "In this paper, TextBlob is used to find the polarity of scraped tweets and \n",
        "\n",
        "Natural Language Toolkit (NLTK) for word frequency [10]. In this paper firstly state-wise analysis is done and then the frequency of Positive, Negative, and Neutral tweets are calculated. Each state is analyzed month-wise separately. Certain insights were discovered about the data using Visualizations [11]\n",
        "\n",
        "As per the frequency plot in Fig. 1 it is observed that there were 61,102 tweets with positive sentiments, 39,195 were having neutral sentiment and 23,987 tweets with negative sentiments in India from Dec 2019 to May 2020. People in India were expressing more positive sentiments as compared to negative and neutral.\n",
        "\n",
        "Analysis of polarity in india from dec 2019 to may 2020.\n",
        "\n",
        "Fig. 2 shows the totals no. of tweets in India from different states. Madhya Pradesh was having more no. of tweets which is 44,252 as compared to any other states in India from Dec 2019 to May 2020. Jammu and Kashmir were on 2nd place with 31,769 tweets. Madhya Pradesh was in 3rd place with 18,577 tweets. Rajasthan was on 4th place with 13,732 tweets from Dec 2019 to May 2019. Mizoram was having only 1 tweet during this period.\n",
        "\n",
        "Month wise analysis of the frequency of tweets in india from dec 2019 to may 2020.\n",
        "\n",
        "Fig. 3 shows that in India there were 56,392 tweets in April 2020 which was much larger than all the months from Dec 2019 to May 2020. and in March there were 45,476 5 which was also greater than any other months except April because at the end of March 2020 Lockdown was announced by the government and people started sharing their views by tweets. So there was a hike in the no. of tweets in May and April 2020.\n",
        "\n",
        "Fig. 4 shows the frequency of positive, negative and neutral tweets in India on lockdown announcement dates. As in the graph, we can see that frequency of tweets on 22nd day is very high which is fair enough because Lockdown in India was also announced in March 2020 so people started sharing their views on this decision of Lockdown taken by the government. And after 22 March 2020, it is observed that the frequency of the tweets was very high than that of the normal days. Fig. 5 is the bar graph is of Maharashtra which shows that there more no. of positive sentiment tweets as compared to neutral and negative sentiment tweets. People started accepting Lockdown and happy with spending their time with their families.\n",
        "\n",
        "Overall sentiments on lockdown and nearby dates.\n",
        "\n",
        "Sentiment frequency plot of maharashtra.\n",
        "\n",
        "The data in table 1 shows that people in Maharashtra were completely ignorant of the Corona Situation until\n",
        "\n",
        "December. They picked up some concern about Corona during January-February, but remained mostly unconcerned. In March, people started to realize the impact of the viral disease and the increase in no. of tweets reflect that.\n",
        "\n",
        "People remained mostly positive about the situation but no. of negative tweets confirm that situation had started taking a serious front in the state and people's psychology.\n",
        "\n",
        "In April, the effect magnified itself to a great extent but more people kept hope, maybe as a result of constant efforts of the government to keep the population away from anxiety. May data tends to show that people were now lesser concerned about the pandemic and more hopeful about the future.\n",
        "\n",
        "Fig. 6 represents the line plot of overall sentiment with polarity in Maharashtra. It is observed that there were more positive sentiment tweets as compared to negative and neutral sentiment tweets.\n",
        "\n",
        "Overall sentiments with polarity in maharashtra.\n",
        "\n",
        "And there is a peak in the frequency on 22 March which is because of the announcement of the Lockdown.\n",
        "\n",
        "In Fig 7 word corona, and COVID was used most frequently used with a frequency count of more than 50,000 in tweets from November 2019 to May 2020. The words country, government, day, anddon was used less than 6000 times. The top 10 words which were used frequently are:-\n",
        "corona,COVID,19,covid19,virus,India,people,amp,lockdown.fight\n",
        "\n",
        "This shows that people's sentiments varied from moderate to highly negative emotions. Overall India spent the months in Lockdown mostly in fear and partly in anguish.\n",
        "\n",
        "Word frequency plot in tweets.\n",
        "\n",
        "The lower end of the graph contains words like\n",
        "like,country,day,today,government,spread,safe\n",
        "\n",
        "The right part of the above graph states a different story. It shows people's emotion in that region varied from health worries and outcomes of the pandemic to political scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "The given challenge is to build a classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. \n",
        "\n",
        "We are given information like Location, Tweet At, Original Tweet, and Sentiment."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.colors import n_colors\n",
        "from plotly.subplots import make_subplots\n",
        "import missingno as msno\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Mounting google colab drive to get access of the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading data\n",
        "Tweet = pd.read_csv('/content/drive/MyDrive/Corona Virus Tweet Sentiment  analysis -Dhrisanda Medhi/Coronavirus Tweets.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "XSmMO4TUeRQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "Tweet.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Last Look\n",
        "Tweet.tail()"
      ],
      "metadata": {
        "id": "e4Ei2-rVfIOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "Tweet.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "Tweet.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "Tweet.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "Tweet.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "msno.bar(Tweet, figsize=(6,3), fontsize=8, color=\"darkblue\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "Tweet.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "Tweet.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The original dataset has 6 columns and 41157 rows. In order to analyze various sentiments, We require just two columns named Original Tweet and Sentiment. There are five types of sentiments- Extremely Negative, Negative, Neutral, Positive, and Extremely Positive as you can see in the following picture.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "Tweet['UserName'].unique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet['ScreenName'].unique()"
      ],
      "metadata": {
        "id": "ITMJ7u_VoN45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet['Location'].unique()"
      ],
      "metadata": {
        "id": "M1fG2iXioUWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet['TweetAt'].unique()"
      ],
      "metadata": {
        "id": "mqxiQDWQoXXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet['OriginalTweet'].unique()"
      ],
      "metadata": {
        "id": "TU9h04SZoxx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet['Sentiment'].unique()"
      ],
      "metadata": {
        "id": "yLwakNUkoydz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code for changing format of date from object to datetime\n",
        "Tweet[\"TweetAt\"] = pd.to_datetime(Tweet[\"TweetAt\"])\n",
        "print(\"Minimum Invoice Date\", min(Tweet[\"TweetAt\"]))\n",
        "print(\"Maximum Invoice Date\", max(Tweet[\"TweetAt\"]))"
      ],
      "metadata": {
        "id": "AkC_BP3bpVn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extract day,month,year"
      ],
      "metadata": {
        "id": "R_bF6O0UwF7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Month extarct\n",
        "Tweet['Month'] = Tweet['TweetAt'].apply(lambda x: x.month)"
      ],
      "metadata": {
        "id": "M3A1Gx7zwHPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#date extract\n",
        "Tweet['Day'] = Tweet['TweetAt'].apply(lambda x: x.day)"
      ],
      "metadata": {
        "id": "QpKfBpsIwMAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#year extract\n",
        "Tweet['Year'] = Tweet['TweetAt'].apply(lambda x: x.year)"
      ],
      "metadata": {
        "id": "xADQo_38wO7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Location"
      ],
      "metadata": {
        "id": "XuKDC5FOgdfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking the count value of different Location \n",
        "Tweet.Location.value_counts().head(15)"
      ],
      "metadata": {
        "id": "11BbKRKogiie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for the unique values in the variable \n",
        "Tweet.Location.unique()\n"
      ],
      "metadata": {
        "id": "aB4TKbuzhQ6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describing the Location \n",
        "Tweet.Location.describe()"
      ],
      "metadata": {
        "id": "5pg0dfH3hpWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No- 2\n",
        "# Looking for top 15 Countries in Countplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(y=Tweet.Location, order = Tweet.Location.value_counts().iloc[:15].index, palette ='icefire')\n",
        "plt.title('Top 15 locations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3C683OBUh2jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above count plot shows the top 15 locations in the dataset.\n",
        "\n",
        "Plot shows the London to be the maximum among all the different locations.\n",
        "\n",
        "Second place is of United States and India ranks at 8th place."
      ],
      "metadata": {
        "id": "gwE0wNgKiDCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataframe for location\n",
        "location = pd.DataFrame(Tweet['Location'].value_counts().sort_values(ascending=False))\n",
        "location = location.rename(columns={'Location':'count'})"
      ],
      "metadata": {
        "id": "Zrr3jibfiPnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required library\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "EOnsPZEziVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No- 3\n",
        "# Plotting the interactive pie plot in percentage of Top 15 locations \n",
        "data = {\n",
        "   \"values\": location['count'][:15],\n",
        "   \"labels\": location.index[:15],\n",
        "   \"domain\": {\"column\": 0},\n",
        "   \"name\": \"Location Name\",\n",
        "   \"hoverinfo\":\"label+percent+name\",\n",
        "   \"hole\": .4,\n",
        "   \"type\": \"pie\"\n",
        "}\n",
        "layout = go.Layout(title=\"<b>Percentage of Location</b>\", legend=dict(x=0.1, y=1.0, orientation=\"v\"))\n",
        "data = [data]\n",
        "fig = go.Figure(data = data, layout = layout)\n",
        "fig.update_layout(title_x=0.5)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "A1pofIWTia5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can again see London has the maximum percentage share of 11.7%, \n",
        "\n",
        "followed by United States with 11.4% and again London, England with 11.2%.\n",
        "\n",
        "India which is placed at 8th place shares the percentage of 5.8%."
      ],
      "metadata": {
        "id": "sqZJC71eik4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "Tweet.UserName.value_counts().sort_values().head(10).plot(kind='bar')\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "Tweet.ScreenName.value_counts().sort_values().head(10).plot(kind='bar')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet.head()"
      ],
      "metadata": {
        "id": "GQFcmEHDe1HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tweet Date"
      ],
      "metadata": {
        "id": "rNN1qS9-mHa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No- 4\n",
        "# Distribution of Dates of Tweets\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='TweetAt', data=Tweet, palette ='Dark2')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Tweeting Dates\")\n",
        "plt.ylabel(\"Count\", fontsize = 12)\n",
        "plt.xlabel(\"TweetAt\",fontsize = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NMiy22EgmSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above count plot shows the overall distribution of different tweeting dates in the dataset. Starting from the mid-month of March 2020 to mid-month of April 2020.\n",
        "\n",
        "From the we can find out that the date 20-03-2020 has the maximum count value among all the other occuring dates.\n",
        "\n",
        "Least count value is for date 28-03-2020.\n",
        "The tweeting date ranges from 16-03-2020 to 14-04-2020, which is approx 30 days in total."
      ],
      "metadata": {
        "id": "on2N60_nmcyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sales_in_month=Tweet['Month'].value_counts().reset_index().rename(columns={'index':'Month','Month':'Sales_count'})\n",
        "# Sales count in different months.\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(x=sales_in_month['Month'],y=sales_in_month['Sales_count'])\n",
        "plt.title('Sales count in different Months ')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sales_in_Day=Tweet['Day'].value_counts().reset_index().rename(columns={'index':'Day','Day':'Sales_count'})\n",
        "# Sales count in different months.\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(x=sales_in_Day['Day'],y=sales_in_Day['Sales_count'])\n",
        "plt.title('Sales count in different inYear ')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentiment"
      ],
      "metadata": {
        "id": "WEvdT1xJmoNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(15,7))\n",
        "sentiment_count = Tweet['Sentiment'].value_counts().reset_index().rename(columns={'index':'Sentiment','Sentiment':'Sentiment_count'})\n",
        "# top 5 countries where max sell happens.\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.barplot(x=sentiment_count['Sentiment'].tail(5),y=sentiment_count['Sentiment_count'].tail(5))\n",
        "plt.title('Sentiment ')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of \"Positive\" sentiments are higher than all other sentiments\n",
        "\n",
        "The above plot is showing the count value of different sentiments present in the dataset.\n",
        "\n",
        "The most common occuring sentiment is positive, followed by negative, neutral, extremely positive and least is extremely negative."
      ],
      "metadata": {
        "id": "BZ9WSuSEzki8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove"
      ],
      "metadata": {
        "id": "GZ39TyqQGo-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate extremely positive/negative values\n",
        "Tweet = Tweet.replace(\"Extremely Positive\", \"Positive\")\n",
        "Tweet = Tweet.replace(\"Extremely Negative\", \"Negative\")\n",
        "\n",
        "Tweet.Sentiment.unique()"
      ],
      "metadata": {
        "id": "vQderPgyGnky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = Tweet.groupby(\"Sentiment\")[\"Sentiment\"].agg(\"count\")\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "\n",
        "counts.plot.bar()"
      ],
      "metadata": {
        "id": "qnFSNbXtJC5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Class Distribution of Each Sentiment Type"
      ],
      "metadata": {
        "id": "zw6wTPjEnMah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying the dataframe\n",
        "tt_df = Tweet.copy()\n",
        "# tt_df['OriginalTweet'] = tt_df['tex"
      ],
      "metadata": {
        "id": "NO7U0A0SnRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the new column 'Text'\n",
        "tt_df['text'] = tt_df.OriginalTweet\n",
        "tt_df[\"text\"] = tt_df[\"text\"].astype(str)"
      ],
      "metadata": {
        "id": "Cyd2b8itnWFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the text count \n",
        "class_df = tt_df.groupby('Sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
        "class_df.style.background_gradient(cmap='viridis')"
      ],
      "metadata": {
        "id": "QjXQ5lj8naxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Number of characters of each sentiments types"
      ],
      "metadata": {
        "id": "1fiv7JPRngtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No - 6\n",
        "# Plotting the number of characters for each tweet sentiment types\n",
        "fig,(ax1,ax2,ax3,ax4,ax5)=plt.subplots(1,5,figsize=(18,5))\n",
        "tweet_len=tt_df[tt_df['Sentiment']==\"Positive\"]['text'].str.len()\n",
        "ax1.hist(tweet_len,color='yellowgreen')\n",
        "ax1.set_title('Positive Sentiments')\n",
        "\n",
        "tweet_len=tt_df[tt_df['Sentiment']==\"Negative\"]['text'].str.len()\n",
        "ax2.hist(tweet_len,color='blue')\n",
        "ax2.set_title('Negative Sentiments')\n",
        "\n",
        "tweet_len=tt_df[tt_df['Sentiment']==\"Neutral\"]['text'].str.len()\n",
        "ax3.hist(tweet_len,color='red')\n",
        "ax3.set_title('Neutral Sentiments')\n",
        "\n",
        "fig.suptitle(\"Characters in Tweet Sentiment\", size=15,fontweight=\"bold\")\n",
        "# Showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MDElYjOHni-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required library\n",
        "from matplotlib.pylab import tile\n",
        "\n",
        "# Writing the function\n",
        "def length(text):    \n",
        "    '''a function which returns the length of text'''\n",
        "    return len(text)\n",
        "tt_df['length'] = tt_df['text'].apply(length)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
        "bins = 150\n",
        "plt.hist(tt_df[tt_df['Sentiment'] == \"Neutral\"]['length'], alpha = 0.5, bins=bins, label='Neutral')\n",
        "plt.hist(tt_df[tt_df['Sentiment'] == \"Positive\"]['length'], alpha = 0.7, bins=bins, label='Positive')\n",
        "plt.hist(tt_df[tt_df['Sentiment'] == \"Negative\"]['length'], alpha = 0.8, bins=bins, label='Negative')\n",
        "plt.hist(tt_df[tt_df['Sentiment'] == \"Extremely Positive\"]['length'], alpha = 0.7, bins=bins, label='Extremely positive')\n",
        "plt.hist(tt_df[tt_df['Sentiment'] == \"Extremely Negative\"]['length'], alpha = 0.8, bins=bins, label='Extremely negative')\n",
        "\n",
        "# Plot No - 7\n",
        "# Plotting the data \n",
        "plt.title('Text Lengths of Each Sentiments in Tweets', fontweight='bold')\n",
        "plt.xlabel('Text Length')\n",
        "plt.ylabel('Text Numbers')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlim(0,200)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "bGZaqcIqoC0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Average word length in a tweet"
      ],
      "metadata": {
        "id": "SRc-GimIn_SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No - 8\n",
        "# Finding the average word length in each tweet sentiment type. \n",
        "fig,(ax1,ax2,ax3,ax4,ax5)=plt.subplots(1,5,figsize=(20,5))\n",
        "word= tt_df[tt_df['Sentiment']==\"Positive\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='yellowgreen')\n",
        "ax1.set_title('Positive')\n",
        "\n",
        "word= tt_df[tt_df['Sentiment']==\"Negative\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='blue')\n",
        "ax2.set_title('Negative')\n",
        "\n",
        "word= tt_df[tt_df['Sentiment']==\"Neutral\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='red')\n",
        "ax3.set_title('Neutral')\n",
        "\n",
        "word= tt_df[tt_df['Sentiment']==\"Extremely Positive\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax4,color='yellow')\n",
        "ax4.set_title('Extremely Positive')\n",
        "\n",
        "word= tt_df[tt_df['Sentiment']==\"Extremely Negative\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
        "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax5,color='darkmagenta')\n",
        "ax5.set_title('Extremely Negative')\n",
        "\n",
        "# Showing the Plot \n",
        "\n",
        "fig.suptitle('Average word length in each tweet', size=15,fontweight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vytiP6OwoLBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Looking occurance times between different tweeting date and sentiments associated with that particular date"
      ],
      "metadata": {
        "id": "A3yNVXxGoSEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot No- 9\n",
        "# Plotting Tweet date with different sentiments\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.histplot(data=Tweet, y= \"TweetAt\", hue=\"Sentiment\", multiple=\"stack\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Tweeting Date of Different Sentiments\", fontweight='bold')\n",
        "plt.ylabel(\"TweetAt\",fontsize = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kv8ewPAVoW0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above plot shows the distribution of different sentiments on the basis of tweeting date (TweetAt - column).\n",
        "\n",
        "Here it can be analysied that date on 20-03-2020, when maximum tweet took place showing the maximum sentiment types. Among all the sentiments, positive sentiment dominates the most followed by the negative in second place.\n",
        "\n",
        "Least tweeting date is 28-03-2020, according to plot, where the number of sentiments is also very less in compare to other dates.\n",
        "\n",
        "The number of extremely positive sentiment tweet can be observed on 25-03-2020.\n",
        "In same way maximum neutral sentiment tweeted on 20-03-2020.\n",
        "\n",
        "Extremely positive sentiment tweeted most on 25-03-2020 whereas extremely negative sentiment tweeted most on 20-03-2020.\n",
        "\n",
        "By seeing all the insight, it can be sensed that this period on 30 days or 1 month, most number of tweets can been seen in the month of March in compare to April month for year 2020.\n",
        "\n",
        "This period is also important because from the month of March globally the number COVID-19 cases started, which triggered many people."
      ],
      "metadata": {
        "id": "KAHnZww4og3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Punctuations Available in Sentiments"
      ],
      "metadata": {
        "id": "kDsj1qt-opBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the corpus from the sentiment & text column and appending them\n",
        "def create_corpus(target):\n",
        "    corpus=[]\n",
        "    \n",
        "    for x in tt_df[tt_df['Sentiment']==target ]['text'].str.split():\n",
        "        for i in x:\n",
        "            corpus.append(i)\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "erI_xV5aos4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required library\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "TTX48GQdoxG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the collection for each sentiment having different punctuation marks for \"Positive Sentiment\" \n",
        "corpus= create_corpus(\"Positive\")\n",
        "\n",
        "dic= defaultdict(int)\n",
        "import string\n",
        "special = string.punctuation\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "        \n",
        "x,y=zip(*dic.items())\n",
        "\n",
        "# Plot No - 10\n",
        "# Plotting the required dataset \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x,y,color='yellowgreen')\n",
        "plt.xlabel('Positive punctuation')\n",
        "plt.title('Punctuation marks in Positive Sentiment',fontweight ='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uFbzhGrvo0zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the collection for each sentiment having different punctuation marks for \"Negative Sentiment\" \n",
        "corpus=create_corpus(\"Negative\")\n",
        "\n",
        "dic=defaultdict(int)\n",
        "import string\n",
        "special = string.punctuation\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "                \n",
        "x,y=zip(*dic.items())\n",
        "\n",
        "# Plot No - 11\n",
        "# Plotting the required dataset \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x,y, color='blue')\n",
        "plt.xlabel('Negative punctuation')\n",
        "plt.title('Punctuation marks in Negative Sentiment',fontweight ='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GYsaHoXdo5zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the collection for each sentiment having different punctuation marks for \"Neutral Sentiment\" \n",
        "corpus=create_corpus(\"Neutral\")\n",
        "\n",
        "dic=defaultdict(int)\n",
        "import string\n",
        "special = string.punctuation\n",
        "for i in (corpus):\n",
        "    if i in special:\n",
        "        dic[i]+=1\n",
        "\n",
        "x,y=zip(*dic.items())\n",
        "\n",
        "# Plot No - 12\n",
        "# Plotting the required dataset \n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(x,y,color='red')\n",
        "plt.xlabel('Neutral punctuation')\n",
        "plt.title('Punctuation marks in Neutral Sentiment',fontweight ='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6h1GmiVho_Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mentions present in Tweets"
      ],
      "metadata": {
        "id": "v_mJkONwpfPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the different mentions present in tweets with having mentions using @ \n",
        "def mentions(text):\n",
        "    line=re.findall(r'(?<=@)\\w+',text)\n",
        "    return \" \".join(line)\n",
        "tt_df['mentions']=tt_df['text'].apply(lambda x:mentions(x))\n",
        "\n",
        "temp=tt_df['mentions'].value_counts()[:][1:11]\n",
        "temp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
        "\n",
        "# Plot No - 15\n",
        "# Ploting the bar plot \n",
        "sns.barplot(x=\"Mentions\",y=\"count\", data = temp)\n",
        "plt.title(\"Top 10 Mentions Present in Tweets\", fontweight='bold')"
      ],
      "metadata": {
        "id": "4ggf3L7dpgmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet.head()"
      ],
      "metadata": {
        "id": "tl_NqgxjJKlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "sns.heatmap(Tweet[['ScreenName','Month','Day','TweetAt','UserName']].corr(),cmap=\"Reds\")\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(Tweet)\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Positive sentimant is increasing when Month is increase"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "v1 = np.random.normal(size=100)\n",
        "v2 = np.random.normal(size=100)\n",
        "\n",
        "res = ttest_ind(v1, v2)\n",
        "\n",
        "print(res)\n",
        "res = ttest_ind(v1, v2).pvalue\n",
        "\n",
        "print(res)\n",
        "# from scipy import stats\n",
        "# rvs = stats.norm.rvs(loc = 5, scale = 10, size = (50,2))\n",
        "# print(stats.ttest_ind(rvs,5.0)\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "P value tells how close to extreme the data actually is.\n",
        "\n",
        "P value and alpha values are compared to establish the statistical significance.\n",
        "\n",
        "If p value <= alpha we reject the null hypothesis and say that the data is statistically significant. otherwise we accept the null hypothesis.\n",
        "\n",
        "T-tests are used to determine if there is significant deference between means of two variables and lets us know if they belong to the same distribution.\n",
        "\n",
        "It is a two tailed test.\n",
        "\n",
        "The function ttest_ind() takes two samples of same size and produces a tuple of t-statistic and p-value.\n",
        "\n",
        "\n",
        "If you want to return only the p-value, use the pvalue property:"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Write your code to make your dataset analysis ready.\n",
        "#Replace nan to 0\n",
        "Tweet=Tweet.fillna(0)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tweet.isna().sum()"
      ],
      "metadata": {
        "id": "O2eDcsBNv7ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "Tweet.describe()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "#For sentiment analysis we only want tweet and sentiment Features\n",
        "df=Tweet[['OriginalTweet','Sentiment']]"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "tpCwWHYl2AVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.lower()\n",
        "df['OriginalTweet']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "df['OriginalTweet'][0]"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df['OriginalTweet'] = df['OriginalTweet'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}